{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f3c257",
   "metadata": {},
   "source": [
    "# LAD Custom experiment\n",
    "\n",
    "Here we test the prediction quality regarding to the existence of spaces in word2vec model\n",
    "\n",
    "In the original implementation the w2v model removes all the numbers and spaces in a log message and create a one single word:\n",
    "\n",
    "`2019-04-12 01:13:20 [DEBUG] Processed 181 out of 181 packages` → `[\"DEBUGProcessedoutofpackages\"]`\n",
    "\n",
    "Here we want to test the original implementation and our approach:\n",
    "\n",
    "`2019-04-12 01:13:20 [DEBUG] Processed 181 out of 181 packages` → `[\"DEBUG\", \"Processed\", \"out\", \"of\", \"packages\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a3f1d4",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31895ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CACHEDIR=/home/nadzya/.cache/matplotlib\n",
      "Using fontManager instance from /home/nadzya/.cache/matplotlib/fontlist-v330.json\n",
      "Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "NumExpr defaulting to 4 threads.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import logging\n",
    "import sompy\n",
    "from multiprocessing import Pool\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim as gs\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad6e191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.disabled = True\n",
    "logging.disable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9038b",
   "metadata": {},
   "source": [
    "# Original Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab84cfc9",
   "metadata": {},
   "source": [
    "### Define Functions\n",
    "\n",
    "#### 1. Log Preprocesing\n",
    "\n",
    "One assumption that all these functions use is that we instantly convert our data into a pandas dataframe that has a \"message\" column containing the relevent information for us. \n",
    "\n",
    "**We then treat each individual log line as a \"word\", cleaning it by removing all non-alphabet charcters including white spaces.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea5833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(data):\n",
    "    for col in data.columns:\n",
    "        if col == \"message\":\n",
    "            data[col] = data[col].apply(_clean_message)\n",
    "        else:\n",
    "            data[col] = data[col].apply(to_str)\n",
    "\n",
    "    data = data.fillna(\"EMPTY\")\n",
    "    \n",
    "def _clean_message(line):\n",
    "    \"\"\"Remove all none alphabetical characters from message strings.\"\"\"\n",
    "    return \"\".join(\n",
    "        re.findall(\"[a-zA-Z]+\", line)\n",
    "    )  # Leaving only a-z in there as numbers add to anomalousness quite a bit\n",
    "\n",
    "def to_str(x):\n",
    "    \"\"\"Convert all non-str lists to string lists for Word2Vec.\"\"\"\n",
    "    ret = \" \".join([str(y) for y in x]) if isinstance(x, list) else str(x)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d684ee",
   "metadata": {},
   "source": [
    "#### 2. Text Encoding  \n",
    "\n",
    "Here we employ the gensim implementation of Word2Vec to encode our logs as fixed length numerical vectors. Logs are noteably not the natural usecase for word2vec, but this appraoch attemps to leverage the fact that logs lines themselves, like words, have a context, so encoding a log based on its co-occurence with other logs does make some intuitive sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3909c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create(words, vector_length, window_size):\n",
    "    \"\"\"Create new word2vec model.\"\"\"\n",
    "    w2vmodel = {}\n",
    "    for col in words.columns:\n",
    "        if col in words:\n",
    "            w2vmodel[col] = gs.models.Word2Vec([list(words[col])], min_count=1, size=vector_length, \n",
    "                                     window=window_size, seed=42, workers=1, iter=550,sg=0)\n",
    "        else:\n",
    "            #_LOGGER.warning(\"Skipping key %s as it does not exist in 'words'\" % col)\n",
    "            pass\n",
    "        \n",
    "    return w2vmodel\n",
    "\n",
    "def one_vector(new_D, w2vmodel):\n",
    "    \"\"\"Create a single vector from model.\"\"\"\n",
    "    transforms = {}\n",
    "    for col in w2vmodel.keys():\n",
    "        if col in new_D:\n",
    "            transforms[col] = w2vmodel[col].wv[new_D[col]]\n",
    "\n",
    "    new_data = []\n",
    "\n",
    "    for i in range(len(transforms[\"message\"])):\n",
    "        logc = np.array(0)\n",
    "        for _, c in transforms.items():\n",
    "            if c.item(i):\n",
    "                logc = np.append(logc, c[i])\n",
    "            else:\n",
    "                logc = np.append(logc, [0, 0, 0, 0, 0])\n",
    "        new_data.append(logc)\n",
    "\n",
    "    return np.array(new_data, ndmin=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca9eecd",
   "metadata": {},
   "source": [
    "#### 3. Model Training\n",
    "\n",
    "Here we employ the SOMPY implementation of the Self-Organizing Map to train our model. This function simply makes it a bit easier for the user to interact with the sompy training requirements. This function returns a trained model.\n",
    "\n",
    "The trained model object also has a method called codebook.matrix() which allows the user access directly to the trained self organizing map itself. If the map successfull converged then it should consist of nodes in our N-dimensional log space that are well ordered and provide an approximation to the topology of the logs in our training set.\n",
    "\n",
    "During training we also, compute the distances of our training data to the trained map as a baseline to build a threashold.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d905148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inp, map_size, iterations, parallelism):\n",
    "    print(f'training dataset is of size {inp.shape[0]}')\n",
    "    mapsize = [map_size, map_size]\n",
    "    np.random.seed(42)\n",
    "    som = sompy.SOMFactory.build(inp, mapsize , initialization='random')\n",
    "    som.train(n_job=parallelism, train_rough_len=100,train_finetune_len=5)\n",
    "    model = som.codebook.matrix.reshape([map_size, map_size, inp.shape[1]])\n",
    "    \n",
    "    #distances = get_anomaly_score(inp, 8, model)\n",
    "    #threshold = 3*np.std(distances) + np.mean(distances)\n",
    "    \n",
    "    return som #,threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaae794",
   "metadata": {},
   "source": [
    "#### 4. Generating Anomaly Scores\n",
    "\n",
    "One of the key elements of this approach is quantifying the distance between our logs and the nodes on our self organizing map. The two functions below, taken together, represent a parrallel implementation for performing this calculaton.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37e35181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anomaly_score(logs, parallelism, model):\n",
    "\n",
    "    parameters = [[x,model] for x in logs]\n",
    "    pool = Pool(parallelism)\n",
    "    dist = pool.map(calculate_anomaly_score, parameters) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return dist\n",
    "\n",
    "def calculate_anomaly_score(parameters):\n",
    "    log = parameters[0]\n",
    "    model = parameters[1]\n",
    "    \"\"\"Compute a distance of a log entry to elements of SOM.\"\"\"\n",
    "    dist_smallest = np.inf\n",
    "    for x in range(model.shape[0]):\n",
    "        for y in range(model.shape[1]):\n",
    "            dist = cosine(model[x][y], log) \n",
    "            #dist = np.linalg.norm(model[x][y] - log)\n",
    "            if dist < dist_smallest:\n",
    "                dist_smallest = dist\n",
    "    return dist_smallest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c6f7d4",
   "metadata": {},
   "source": [
    "#### 5. Model Inference / Prediction\n",
    "\n",
    "Here we are making an inference about a new log message. This is done by scoring the incoming log and evaluating whether or not it passess a certain threshold value.  \n",
    "\n",
    "\n",
    "Ideally our word2vec has been monitoring our application long enough to have seen all the logs. So, if we get a known log we can simply look up its vector representation   \n",
    "\n",
    "One downside with word2vec is that its quite brittle when it comes to incorporating words that haven't been seen before. In this example, we will retrain the W2Vmodel if our new log has not been seen by the before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eba2949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(w2v, som, log, data, threshold):\n",
    "    \n",
    "    log =  pd.DataFrame({\"message\":log},index=[1])\n",
    "    _preprocess(log)\n",
    "    \n",
    "    if log.message.iloc[0] in list(w2v['message'].wv.vocab.keys()):\n",
    "        vector = w2v[\"message\"].wv[log.message.iloc[0]]\n",
    "    else:\n",
    "        w2v = gs.models.Word2Vec([[log.message.iloc[0]] + list(data[\"message\"])], \n",
    "                                 min_count=1, size=25, window=3, seed=42, workers=1, iter=550, sg=0)\n",
    "        vector = w2v.wv[log.message.iloc[0]]\n",
    "    \n",
    "    score = get_anomaly_score([vector], 1, som)\n",
    "    \n",
    "    if score < threshold:\n",
    "        return 0, score\n",
    "    else:\n",
    "        return 1, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137156f3",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e445de",
   "metadata": {},
   "source": [
    "### Get logs from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29f13e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;158&gt;Nov 25 12:02:31 195-137-160-145 nginx-acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;158&gt;Nov 25 12:04:11 195-137-160-145 nginx-acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;158&gt;Nov 25 12:15:42 195-137-160-145 nginx-acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;158&gt;Nov 25 12:25:32 195-137-160-145 nginx-acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;158&gt;Nov 25 12:25:22 195-137-160-145 nginx-acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>&lt;158&gt;Nov 25 16:01:44 195-137-160-145 nginx-acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>&lt;158&gt;Nov 25 13:53:47 195-137-160-145 nginx-acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>&lt;158&gt;Nov 25 15:25:02 195-137-160-145 nginx-acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>&lt;158&gt;Nov 25 14:48:50 195-137-160-145 nginx-acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>&lt;158&gt;Nov 25 16:01:44 195-137-160-145 nginx-acc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                message\n",
       "0     <158>Nov 25 12:02:31 195-137-160-145 nginx-acc...\n",
       "1     <158>Nov 25 12:04:11 195-137-160-145 nginx-acc...\n",
       "2     <158>Nov 25 12:15:42 195-137-160-145 nginx-acc...\n",
       "3     <158>Nov 25 12:25:32 195-137-160-145 nginx-acc...\n",
       "4     <158>Nov 25 12:25:22 195-137-160-145 nginx-acc...\n",
       "...                                                 ...\n",
       "9995  <158>Nov 25 16:01:44 195-137-160-145 nginx-acc...\n",
       "9996  <158>Nov 25 13:53:47 195-137-160-145 nginx-acc...\n",
       "9997  <158>Nov 25 15:25:02 195-137-160-145 nginx-acc...\n",
       "9998  <158>Nov 25 14:48:50 195-137-160-145 nginx-acc...\n",
       "9999  <158>Nov 25 16:01:44 195-137-160-145 nginx-acc...\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = r\"file:///home/nadzya/Apps/log-anomaly-detector/validation_data/solidex.by.json\"\n",
    "data = pd.DataFrame(pd.read_json(data_path, orient=str).message)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6813d0f",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f497451b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NovnginxaccessNovGETHTTPGohttpclient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NovnginxaccessNovGETcoursesHTTPhttpwwwsolidexb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NovnginxaccessNovGETHTTPGohttpclient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NovnginxaccessNovGETHTTPGohttpclient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NovnginxaccessNovGETHTTPGohttpclient</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message\n",
       "0               NovnginxaccessNovGETHTTPGohttpclient\n",
       "1  NovnginxaccessNovGETcoursesHTTPhttpwwwsolidexb...\n",
       "2               NovnginxaccessNovGETHTTPGohttpclient\n",
       "3               NovnginxaccessNovGETHTTPGohttpclient\n",
       "4               NovnginxaccessNovGETHTTPGohttpclient"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = data.copy()\n",
    "_preprocess(preprocessed_data)\n",
    "\n",
    "# First 5 prepocessed messages\n",
    "pd.DataFrame(preprocessed_data.message).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b642f7f",
   "metadata": {},
   "source": [
    "Let's see how many logs are there for each preprocessed word-log. We display top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df13ecf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NovnginxaccessNovGETokompaniiHTTPHealthCheck 3979\n",
      "NovnginxaccessNovGETHTTPGohttpclient 3836\n",
      "NovnginxaccessNovPOSTwpcronphpdoingwpcronHTTPWordPresshttpwwwsolidexby 734\n",
      "NovnginxaccessNovPOSTwploginphpHTTPhttpwwwsolidexbywploginphpMozillaWindowsNTWinxrvGeckoFirefox 702\n",
      "NovnginxaccessNovHEADHTTPhttpsolidexbyMozillacompatibleUptimeRobothttpwwwuptimerobotcom 376\n"
     ]
    }
   ],
   "source": [
    "x = preprocessed_data.message.value_counts()\n",
    "for i in x.keys()[:5]:\n",
    "    print(i, x[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e5cda",
   "metadata": {},
   "source": [
    "### Word2Vec for such words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38c313ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = create(words=preprocessed_data, vector_length=25, window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4e018af",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_vectors = one_vector(preprocessed_data, w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cbea65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 26)\n",
      "(10000, 25)\n"
     ]
    }
   ],
   "source": [
    "print(log_vectors.shape)\n",
    "print(log_vectors[:, 1:].shape)\n",
    "log_vectors = log_vectors[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0158dee",
   "metadata": {},
   "source": [
    "### Train SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a751ce2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset is of size 10000\n"
     ]
    }
   ],
   "source": [
    "map_size = 24\n",
    "som = train(log_vectors, map_size=map_size, iterations=0, parallelism=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85b8862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = som.codebook.matrix.reshape([map_size, map_size, log_vectors.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c876aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_scores = get_anomaly_score(log_vectors, parallelism=4, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8076d660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in anomaly_scores if x > 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fbe6102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8403589377654117"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 3*np.std(anomaly_scores) + np.mean(anomaly_scores)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25595b32",
   "metadata": {},
   "source": [
    "# With Custom Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3460fc",
   "metadata": {},
   "source": [
    "### Define Functions\n",
    "\n",
    "#### 1. Log Preprocesing\n",
    "\n",
    "One assumption that all these functions use is that we instantly convert our data into a pandas dataframe that has a \"message\" column containing the relevent information for us. \n",
    "\n",
    "We then treat each individual log line as a set of words, cleaning it by removing all non-alphabet charcters.\n",
    "\n",
    "We keep white spaces and return a list of wordss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22d847e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_custom(data):\n",
    "    for col in data.columns:\n",
    "        if col == \"message\":\n",
    "            data[col] = data[col].apply(_clean_message_custom)\n",
    "        else:\n",
    "            data[col] = data[col].apply(to_str_custom)\n",
    "\n",
    "    data = data.fillna(\"EMPTY\")\n",
    "    \n",
    "def _clean_message_custom(line):\n",
    "    \"\"\"Remove all none alphabetical characters from message strings.\"\"\"\n",
    "    words = list(re.findall(\"[a-zA-Z]+\", line))\n",
    "    return words\n",
    "\n",
    "def to_str_custom(x):\n",
    "    \"\"\"Convert all non-str lists to string lists for Word2Vec.\"\"\"\n",
    "    ret = \" \".join([str(y) for y in x]) if isinstance(x, list) else str(x)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf0513",
   "metadata": {},
   "source": [
    "#### Text encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6d882a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom(logs, vector_length, window_size):\n",
    "    \"\"\"Create new word2vec model.\"\"\"\n",
    "    model = gs.models.Word2Vec(sentences=list(logs), size=vector_length, window=window_size)\n",
    "    return model\n",
    "\n",
    "def get_vectors(model, logs, vector_length):\n",
    "    \"\"\"Return logs as list of vectorized words\"\"\"\n",
    "    vectors = []\n",
    "    for x in logs:\n",
    "        temp = []\n",
    "        for word in x:\n",
    "            if word in model.wv:\n",
    "                temp.append(model.wv[word])\n",
    "            else:\n",
    "                temp.append(np.array([0]*vector_length))\n",
    "        vectors.append(temp)\n",
    "    return vectors\n",
    "\n",
    "def _log_words_to_one_vector(log_words_vectors):\n",
    "        result = []\n",
    "        log_array_transposed = np.array(log_words_vectors, dtype=object).transpose()\n",
    "        for coord in log_array_transposed:\n",
    "            result.append(np.mean(coord))\n",
    "        return result\n",
    "\n",
    "def vectorized_logs_to_single_vectors(vectors):\n",
    "    \"\"\"Represent log messages as vectors according to the vectors\n",
    "    of the words in these logs\n",
    "\n",
    "    :params vectors: list of log messages, represented as list of words vectors\n",
    "            [[wordvec11, wordvec12], [wordvec21, wordvec22], ...]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for log_words_vector in vectors:\n",
    "        result.append(_log_words_to_one_vector(log_words_vector))\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d808d557",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "709af82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_custom(w2v, som, log, logs_list, threshold):\n",
    "    \n",
    "    log = pd.DataFrame({\"message\": log}, index=[1])\n",
    "    _preprocess_custom(log)\n",
    "    \n",
    "    vector = []\n",
    "    w2v = gs.models.Word2Vec([log.message.iloc[0]] + logs_list,\n",
    "                             min_count=1, size=25, window=5)\n",
    "    for word in log.message.iloc[0]:\n",
    "        if word in w2v.wv.vocab.keys():\n",
    "            vector.append(w2v.wv[word])\n",
    "        else:\n",
    "            vector.append(np.array([0]*25))\n",
    "    \n",
    "    one_vector = _log_words_to_one_vector(vector)\n",
    "    \n",
    "    score = get_anomaly_score([one_vector], 1, som)\n",
    "    \n",
    "    if score < threshold:\n",
    "        return 0, score\n",
    "    else:\n",
    "        return 1, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981dfda9",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344685a",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f38604cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Nov, nginx, access, Nov, GET, HTTP, Go, http,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Nov, nginx, access, Nov, GET, courses, HTTP, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Nov, nginx, access, Nov, GET, HTTP, Go, http,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Nov, nginx, access, Nov, GET, HTTP, Go, http,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Nov, nginx, access, Nov, GET, HTTP, Go, http,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message\n",
       "0  [Nov, nginx, access, Nov, GET, HTTP, Go, http,...\n",
       "1  [Nov, nginx, access, Nov, GET, courses, HTTP, ...\n",
       "2  [Nov, nginx, access, Nov, GET, HTTP, Go, http,...\n",
       "3  [Nov, nginx, access, Nov, GET, HTTP, Go, http,...\n",
       "4  [Nov, nginx, access, Nov, GET, HTTP, Go, http,..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_preproc_data = data.copy()\n",
    "_preprocess_custom(custom_preproc_data)\n",
    "\n",
    "# First 5 prepocessed messages\n",
    "pd.DataFrame(custom_preproc_data.message).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b13234ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nov', 'nginx', 'access', 'Nov', 'GET', 'HTTP', 'Go', 'http', 'client']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_list = list(custom_preproc_data.message)\n",
    "logs_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e5171d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_custom = create_custom(logs_list, vector_length=25, window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faa5b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_custom = get_vectors(model=w2v_custom, logs=logs_list, vector_length=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf6578a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_as_vectors = vectorized_logs_to_single_vectors(vectors_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d0bd77",
   "metadata": {},
   "source": [
    "### Train SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14766836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset is of size 10000\n"
     ]
    }
   ],
   "source": [
    "map_size = 24\n",
    "som_custom = train(logs_as_vectors, map_size=map_size, iterations=0, parallelism=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abf4872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom = som_custom.codebook.matrix.reshape([map_size, map_size, logs_as_vectors.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f3b4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_scores_custom = get_anomaly_score(logs_as_vectors, parallelism=4, model=model_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "838417e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6063197258670838,\n",
       " 0.6063197258670838,\n",
       " 0.7999504675137618,\n",
       " 0.7999504675137618,\n",
       " 0.7999504675137618,\n",
       " 0.7999504675137618,\n",
       " 0.8141560154503695,\n",
       " 0.6685166572869321,\n",
       " 0.8141560154503695,\n",
       " 0.6685166572869321,\n",
       " 0.8141560154503695,\n",
       " 0.6685166572869321,\n",
       " 0.8141560154503695,\n",
       " 0.7572923941418215,\n",
       " 0.7572923941418215,\n",
       " 0.7572923941418215,\n",
       " 0.7572923941418215,\n",
       " 0.7572923941418215,\n",
       " 0.7572923941418215,\n",
       " 0.7572923941418215,\n",
       " 0.7572923941418215,\n",
       " 0.7572923941418215,\n",
       " 0.7572923941418215,\n",
       " 0.7999504675137618,\n",
       " 0.7999504675137618]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in anomaly_scores_custom if x > 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5e804aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8403589377654117"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold_custom = 3*np.std(anomaly_scores) + np.mean(anomaly_scores)\n",
    "threshold_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7120e68c",
   "metadata": {},
   "source": [
    "# Prediciton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d34b892",
   "metadata": {},
   "source": [
    "**This is a test message**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d38c106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [0.14252834281546678])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(w2v, model, \"This is a test message\", preprocessed_data, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86c92235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [0.6412396233086541])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_custom(w2v_custom, model_custom, \"This is a test message\", logs_list, threshold_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfff9f4c",
   "metadata": {},
   "source": [
    "**<182>Dec 07 14:16:16 dataform  172.17.17.100 - - [07/Dec/2021:14:16:16 +0300] \"POST /cgi-bin/.%2e/.%2e/.%2e/.%2e/etc/passwd HTTP/1.1\" 400 5604 \"-\" \"curl/7.68.0\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f12b7302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [0.13615576457271295])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(w2v, model, \"<182>Dec 07 14:16:16 dataform  172.17.17.100 - - [07/Dec/2021:14:16:16 +0300] \\\"POST /cgi-bin/.%2e/.%2e/.%2e/.%2e/etc/passwd HTTP/1.1\\\" 400 5604 \\\"-\\\" \\\"curl/7.68.0\\\"\", preprocessed_data, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0cfd3624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, [0.8640913019388633])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_custom(w2v_custom, model_custom, \"<182>Dec 07 14:16:16 dataform  172.17.17.100 - - [07/Dec/2021:14:16:16 +0300] \\\"POST /cgi-bin/.%2e/.%2e/.%2e/.%2e/etc/passwd HTTP/1.1\\\" 400 5604 \\\"-\\\" \\\"curl/7.68.0\\\"\", logs_list, threshold_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75592338",
   "metadata": {},
   "source": [
    "**<179>Dec 07 14:51:17 dataform  [Wed Dec 07 14:51:17.120946 2021] [auth_basic:error] [pid 2179734:tid 139663791892224] [client 172.17.17.100:44992] AH01617: user webadmin: authentication failure for \"/register\": Password Mismatch***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf6462dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [0.1326183988460914])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(w2v, model, \"<179>Dec 07 14:51:17 dataform  [Wed Dec 07 14:51:17.120946 2021] [auth_basic:error] [pid 2179734:tid 139663791892224] [client 172.17.17.100:44992] AH01617: user webadmin: authentication failure for \\\"/register\\\": Password Mismatch\", preprocessed_data, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a767da9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [0.8363002343084958])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_custom(w2v_custom, model_custom, \"<179>Dec 07 14:51:17 dataform  [Wed Dec 07 14:51:17.120946 2021] [auth_basic:error] [pid 2179734:tid 139663791892224] [client 172.17.17.100:44992] AH01617: user webadmin: authentication failure for \\\"/register\\\": Password Mismatch\", logs_list, threshold_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a6efc5",
   "metadata": {},
   "source": [
    "**<179>Dec 07 12:10:53 smtplib.SMTPRecipientsRefused: {'doesntexist@solidex.by': (550, b'5.1.1 <doesntexist@solidex.by>: Recipient address rejected: User unknown in virtual mailbox table')}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e888a0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [0.14048444087710865])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(w2v, model, \"<179>Dec 07 12:10:53 smtplib.SMTPRecipientsRefused: {'doesntexist@solidex.by': (550, b'5.1.1 <doesntexist@solidex.by>: Recipient address rejected: User unknown in virtual mailbox table')}\", preprocessed_data, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e82eea70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [0.45630292906224423])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_custom(w2v_custom, model_custom, \"<179>Dec 07 12:10:53 smtplibb.SMTPRecipientsRefused: {'doesntexist@solidex.by': (550, b'5.1.1 <doesntexist@solidex.by>: Recipient address rejected: User unknown in virtual mailbox table')}\", logs_list, threshold_custom)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
