{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2a0b2e5",
   "metadata": {},
   "source": [
    "# AutoEncoder vs Local Outlier Factor\n",
    "\n",
    "Here we want to understand if AE can be more efficient than LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed03e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim as gs\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc662ea",
   "metadata": {},
   "source": [
    "# Define functions\n",
    "\n",
    "#### 1. Log Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f91dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    for col in data.columns:\n",
    "        if col == \"message\":\n",
    "            data[col] = data[col].apply(clean_message)\n",
    "        else:\n",
    "            data[col] = data[col].apply(to_str)\n",
    "\n",
    "    data = data.fillna(\"EMPTY\")\n",
    "    \n",
    "def clean_message(line):\n",
    "    \"\"\"Remove all none alphabetical characters from message strings.\"\"\"\n",
    "    words = list(re.findall(\"[a-zA-Z]+\", line))\n",
    "    return words\n",
    "\n",
    "def to_str(x):\n",
    "    \"\"\"Convert all non-str lists to string lists for Word2Vec.\"\"\"\n",
    "    ret = \" \".join([str(y) for y in x]) if isinstance(x, list) else str(x)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4887f",
   "metadata": {},
   "source": [
    "#### 2. Text encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6658935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create(logs, vector_length, window_size):\n",
    "    \"\"\"Create new word2vec model.\"\"\"\n",
    "    model = gs.models.Word2Vec(sentences=list(logs), size=vector_length, window=window_size)\n",
    "    return model\n",
    "\n",
    "def get_vectors(model, logs, vector_length):\n",
    "    \"\"\"Return logs as list of vectorized words\"\"\"\n",
    "    vectors = []\n",
    "    for x in logs:\n",
    "        temp = []\n",
    "        for word in x:\n",
    "            if word in model.wv:\n",
    "                temp.append(model.wv[word])\n",
    "            else:\n",
    "                temp.append(np.array([0]*vector_length))\n",
    "        vectors.append(temp)\n",
    "    return vectors\n",
    "\n",
    "def _log_words_to_one_vector(log_words_vectors):\n",
    "        result = []\n",
    "        log_array_transposed = np.array(log_words_vectors, dtype=object).transpose()\n",
    "        for coord in log_array_transposed:\n",
    "            result.append(np.mean(coord))\n",
    "        return result\n",
    "\n",
    "def vectorized_logs_to_single_vectors(vectors):\n",
    "    \"\"\"Represent log messages as vectors according to the vectors\n",
    "    of the words in these logs\n",
    "\n",
    "    :params vectors: list of log messages, represented as list of words vectors\n",
    "            [[wordvec11, wordvec12], [wordvec21, wordvec22], ...]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for log_words_vector in vectors:\n",
    "        result.append(_log_words_to_one_vector(log_words_vector))\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaab60a",
   "metadata": {},
   "source": [
    "#### 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3bcbca",
   "metadata": {},
   "source": [
    "Time measure decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8db520ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print('%r  %2.2f ms' % \\\n",
    "                  (method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc2f567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def train_lof(X, n_neighbors, metric):\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, metric=metric)\n",
    "    pred = lof.fit_predict(X)\n",
    "    lof_model = LocalOutlierFactor(n_neighbors=n_neighbors, metric=metric, novelty=True)\n",
    "    lof_model.fit(X)\n",
    "    return pred, lof_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84022c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(Model):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_units: int\n",
    "      Number of output units\n",
    "  \n",
    "    code_size: int\n",
    "      Number of units in bottle neck\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_units, code_size=8):\n",
    "        super().__init__()\n",
    "        self.encoder = Sequential([\n",
    "          Dense(64, activation='relu'),\n",
    "          Dropout(0.1),\n",
    "          Dense(32, activation='relu'),\n",
    "          Dropout(0.1),\n",
    "          Dense(16, activation='relu'),\n",
    "          Dropout(0.1),\n",
    "          Dense(code_size, activation='relu')\n",
    "        ])\n",
    "        self.decoder = Sequential([\n",
    "          Dense(16, activation='relu'),\n",
    "          Dropout(0.1),\n",
    "          Dense(32, activation='relu'),\n",
    "          Dropout(0.1),\n",
    "          Dense(64, activation='relu'),\n",
    "          Dropout(0.1),\n",
    "          Dense(output_units, activation='sigmoid')\n",
    "        ])\n",
    "  \n",
    "    def call(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    @timeit\n",
    "    def fitit(self, *args, **kwargs):\n",
    "        super().fit(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7d9d5c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold(model, x_train_scaled):\n",
    "    reconstructions = model.predict(x_train_scaled)\n",
    "    # provides losses of individual instances\n",
    "    reconstruction_errors = tensorflow.keras.losses.msle(reconstructions, x_train_scaled)\n",
    "    # threshold for anomaly scores\n",
    "    threshold = np.mean(reconstruction_errors.numpy()) \\\n",
    "                + np.std(reconstruction_errors.numpy())\n",
    "    return threshold\n",
    "\n",
    "def get_predictions(model, x_test_scaled, threshold):\n",
    "    predictions = model.predict(x_test_scaled)\n",
    "    # provides losses of individual instances\n",
    "    errors = tensorflow.keras.losses.msle(predictions, x_test_scaled)\n",
    "    # 1 = anomaly, 0 = normal\n",
    "    anomaly_mask = pd.Series(errors) > threshold\n",
    "    preds = anomaly_mask.map(lambda x: 1 if x == True else 0)\n",
    "    return preds, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99f6f3",
   "metadata": {},
   "source": [
    "#### 4. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ce9f217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_lof(log, lof, loglist):\n",
    "    log = pd.DataFrame({\"message\": log}, index=[1])\n",
    "    preprocess(log)\n",
    "    \n",
    "    vector = []\n",
    "    w2v = gs.models.Word2Vec([log.message.iloc[0]] + loglist,\n",
    "                             min_count=1, size=25, window=5)\n",
    "    for word in log.message.iloc[0]:\n",
    "        if word in w2v.wv.vocab.keys():\n",
    "            vector.append(w2v.wv[word])\n",
    "        else:\n",
    "            vector.append(np.array([0]*25))\n",
    "    one_vector = _log_words_to_one_vector(vector)\n",
    "    pred = lof.predict([one_vector])\n",
    "    score = abs(lof.score_samples([one_vector])[0])\n",
    "    if pred[0] == -1:\n",
    "        return 1, score\n",
    "    return 0, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c325c77",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a4cadf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"file:///home/nadzya/Apps/log-anomaly-detector/validation_data/slx.json\"\n",
    "data = pd.DataFrame(pd.read_json(data_path, orient=str).message).iloc[:10000]\n",
    "\n",
    "preprocessed_data = data.copy()\n",
    "preprocess(preprocessed_data)\n",
    "\n",
    "logs_list = list(preprocessed_data.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5d4303e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v = create(logs_list, vector_length=25, window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "69c2ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = get_vectors(model=w2v, logs=logs_list, vector_length=25)\n",
    "logs_as_vectors = vectorized_logs_to_single_vectors(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f82c32",
   "metadata": {},
   "source": [
    "## LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ad804",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, lof = train_lof(logs_as_vectors, 100, 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25358c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_lof = []\n",
    "for x in pred:\n",
    "    if x == 1:\n",
    "        anomaly_lof.append(0)\n",
    "    else:\n",
    "        anomaly_lof.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a4832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "100*len([x for x in anomaly_lof if x == 1])/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba49095",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lof = data.copy()\n",
    "data_lof[\"anomaly\"] = anomaly_lof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbf76aa",
   "metadata": {},
   "source": [
    "## AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2653595",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "logs_scaled = min_max_scaler.fit_transform(logs_as_vectors.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2716d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = AutoEncoder(output_units=logs_scaled.shape[1])\n",
    "ae.compile(loss='msle', metrics=['mse'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cd8fce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ae_result = ae.fitit(logs_scaled, logs_scaled, epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e1aa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = find_threshold(ae, logs_scaled)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1372bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, ae_errors = get_predictions(ae, logs_scaled, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7719f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "100*len(predictions.loc[predictions == 1])/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ce3cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ae = data.copy()\n",
    "data_ae[\"anomaly\"] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba8a538",
   "metadata": {},
   "source": [
    "## AE vs LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a11d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_anomaly_msgs = list(data_lof.loc[data_lof[\"anomaly\"] == 1].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60809061",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ae_anomaly_msgs = list(data_ae.loc[data_ae[\"anomaly\"] == 1].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0129eb56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lof_diff_logs = []\n",
    "ae_diff_logs = []\n",
    "for x in list(set(ae_anomaly_msgs) - set(lof_anomaly_msgs)) + list(set(lof_anomaly_msgs) - set(ae_anomaly_msgs)) :\n",
    "    if (x in lof_anomaly_msgs) and (not x in ae_anomaly_msgs):\n",
    "        lof_diff_logs.append(x)\n",
    "    if x in ae_anomaly_msgs and not x in lof_anomaly_msgs:\n",
    "        ae_diff_logs.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed153ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total logs\", len(data))\n",
    "print(\"Anomalies, detected by LOF:\", len(lof_anomaly_msgs))\n",
    "print(\"Anomaly percentage, detected by LOF:\", len(lof_anomaly_msgs)*100/len(data), \"%\")\n",
    "\n",
    "print(\"Anomalies, detected by AE:\", len(ae_anomaly_msgs))\n",
    "print(\"Anomaly percentage, detected by AE:\", len(ae_anomaly_msgs)*100/len(data), \"%\")\n",
    "\n",
    "print(\"Number of messages, that LOF detected, but AE did not:\", len(lof_diff_logs))\n",
    "print(\"Number of messages, that AE detected, but LOF did not:\", len(ae_diff_logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8328392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lof_diff_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9129ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ae_diff_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c752a916",
   "metadata": {},
   "source": [
    "### The result\n",
    "\n",
    "AE detects more anomaly messages than LOF, but some of these messages are not actually anomalies.\n",
    "\n",
    "Both LOF and AE detects the same types of anomaly messages, but AE marks all messages of such type (for example, login failed) as anomalies. In contrast, LOF detects only part of such messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a394301b",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956851b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_scores = abs(lof.score_samples(logs_as_vectors))\n",
    "lof_scores_normalized = lof_scores/max(lof_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca005f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_ae_scores = list(zip(list(lof_scores), list(map(float, ae_errors))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b38332",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be2ce0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ensemble_anomalies = []\n",
    "for i in range(len(anomaly_lof)):\n",
    "    if ae_errors[i] > threshold and lof_scores[i] > 1:\n",
    "        ensemble_anomalies.append(data.iloc[i].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dde359",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "set(ae_anomaly_msgs) - set(ensemble_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd53fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ensemble_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd13de14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
