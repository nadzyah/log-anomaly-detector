{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "757b1ddf",
   "metadata": {},
   "source": [
    "# AutoEncoder and Generative Adversarial Networks\n",
    "\n",
    "Here we want to understand if AE or GAN can be more efficient than LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebc1fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim as gs\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a2368",
   "metadata": {},
   "source": [
    "# Define functions\n",
    "\n",
    "#### 1. Log Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf8e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    for col in data.columns:\n",
    "        if col == \"message\":\n",
    "            data[col] = data[col].apply(clean_message)\n",
    "        else:\n",
    "            data[col] = data[col].apply(to_str)\n",
    "\n",
    "    data = data.fillna(\"EMPTY\")\n",
    "    \n",
    "def clean_message(line):\n",
    "    \"\"\"Remove all none alphabetical characters from message strings.\"\"\"\n",
    "    words = list(re.findall(\"[a-zA-Z]+\", line))\n",
    "    return words\n",
    "\n",
    "def to_str(x):\n",
    "    \"\"\"Convert all non-str lists to string lists for Word2Vec.\"\"\"\n",
    "    ret = \" \".join([str(y) for y in x]) if isinstance(x, list) else str(x)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc4fdf1",
   "metadata": {},
   "source": [
    "#### 2. Text encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4099746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create(logs, vector_length, window_size):\n",
    "    \"\"\"Create new word2vec model.\"\"\"\n",
    "    model = gs.models.Word2Vec(sentences=list(logs), size=vector_length, window=window_size)\n",
    "    return model\n",
    "\n",
    "def get_vectors(model, logs, vector_length):\n",
    "    \"\"\"Return logs as list of vectorized words\"\"\"\n",
    "    vectors = []\n",
    "    for x in logs:\n",
    "        temp = []\n",
    "        for word in x:\n",
    "            if word in model.wv:\n",
    "                temp.append(model.wv[word])\n",
    "            else:\n",
    "                temp.append(np.array([0]*vector_length))\n",
    "        vectors.append(temp)\n",
    "    return vectors\n",
    "\n",
    "def _log_words_to_one_vector(log_words_vectors):\n",
    "        result = []\n",
    "        log_array_transposed = np.array(log_words_vectors, dtype=object).transpose()\n",
    "        for coord in log_array_transposed:\n",
    "            result.append(np.mean(coord))\n",
    "        return result\n",
    "\n",
    "def vectorized_logs_to_single_vectors(vectors):\n",
    "    \"\"\"Represent log messages as vectors according to the vectors\n",
    "    of the words in these logs\n",
    "\n",
    "    :params vectors: list of log messages, represented as list of words vectors\n",
    "            [[wordvec11, wordvec12], [wordvec21, wordvec22], ...]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for log_words_vector in vectors:\n",
    "        result.append(_log_words_to_one_vector(log_words_vector))\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60279fd8",
   "metadata": {},
   "source": [
    "#### 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dfb0c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lof(X, eps, min_samples):\n",
    "    lof = LocalOutlierFactor(metric='euclidean')\n",
    "    pred = lof.fit_predict(X)\n",
    "    lof_model = LocalOutlierFactor(metric='euclidean', novelty=True)\n",
    "    lof_model.fit(X)\n",
    "    return pred, lof_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccfae080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(Model):\n",
    "  \"\"\"\n",
    "  Parameters\n",
    "  ----------\n",
    "  output_units: int\n",
    "    Number of output units\n",
    "  \n",
    "  code_size: int\n",
    "    Number of units in bottle neck\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, output_units, code_size=8):\n",
    "    super().__init__()\n",
    "    self.encoder = Sequential([\n",
    "      Dense(64, activation='relu'),\n",
    "      Dropout(0.1),\n",
    "      Dense(32, activation='relu'),\n",
    "      Dropout(0.1),\n",
    "      Dense(16, activation='relu'),\n",
    "      Dropout(0.1),\n",
    "      Dense(code_size, activation='relu')\n",
    "    ])\n",
    "    self.decoder = Sequential([\n",
    "      Dense(16, activation='relu'),\n",
    "      Dropout(0.1),\n",
    "      Dense(32, activation='relu'),\n",
    "      Dropout(0.1),\n",
    "      Dense(64, activation='relu'),\n",
    "      Dropout(0.1),\n",
    "      Dense(output_units, activation='sigmoid')\n",
    "    ])\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    encoded = self.encoder(inputs)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1d1b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold(model, x_train_scaled):\n",
    "    reconstructions = model.predict(x_train_scaled)\n",
    "    # provides losses of individual instances\n",
    "    reconstruction_errors = tensorflow.keras.losses.msle(reconstructions, x_train_scaled)\n",
    "    # threshold for anomaly scores\n",
    "    threshold = np.mean(reconstruction_errors.numpy()) \\\n",
    "                + np.std(reconstruction_errors.numpy())\n",
    "    return threshold\n",
    "\n",
    "def get_predictions(model, x_test_scaled, threshold):\n",
    "    predictions = model.predict(x_test_scaled)\n",
    "    # provides losses of individual instances\n",
    "    errors = tensorflow.keras.losses.msle(predictions, x_test_scaled)\n",
    "    # 1 = anomaly, 0 = normal\n",
    "    anomaly_mask = pd.Series(errors) > threshold\n",
    "    preds = anomaly_mask.map(lambda x: 1.0 if x == True else 0.0)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95430f",
   "metadata": {},
   "source": [
    "#### 4. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdf7ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_lof(log, lof, loglist):\n",
    "    log = pd.DataFrame({\"message\": log}, index=[1])\n",
    "    preprocess(log)\n",
    "    \n",
    "    vector = []\n",
    "    w2v = gs.models.Word2Vec([log.message.iloc[0]] + loglist,\n",
    "                             min_count=1, size=25, window=5)\n",
    "    for word in log.message.iloc[0]:\n",
    "        if word in w2v.wv.vocab.keys():\n",
    "            vector.append(w2v.wv[word])\n",
    "        else:\n",
    "            vector.append(np.array([0]*25))\n",
    "    one_vector = _log_words_to_one_vector(vector)\n",
    "    pred = lof.predict([one_vector])\n",
    "    score = abs(lof.score_samples([one_vector])[0])\n",
    "    if pred[0] == -1:\n",
    "        return 1, score\n",
    "    return 0, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d122765c",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d94d4b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"file:///home/nadzya/Apps/log-anomaly-detector/validation_data/slx.json\"\n",
    "data = pd.DataFrame(pd.read_json(data_path, orient=str).message)\n",
    "\n",
    "preprocessed_data = data.copy()\n",
    "preprocess(preprocessed_data)\n",
    "\n",
    "logs_list = list(preprocessed_data.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "75a38b17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v = create(logs_list, vector_length=25, window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "214ecb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = get_vectors(model=w2v, logs=logs_list, vector_length=25)\n",
    "logs_as_vectors = vectorized_logs_to_single_vectors(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b349acfd",
   "metadata": {},
   "source": [
    "## LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "14c01b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, lof = train_lof(logs_as_vectors, 0.5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "6bf126a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_lof = []\n",
    "for x in pred:\n",
    "    if x == 1:\n",
    "        anomaly_lof.append(0)\n",
    "    else:\n",
    "        anomaly_lof.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "77b9f92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.28"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*len([x for x in anomaly_lof if x == 1])/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "11c30c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lof = data.copy()\n",
    "data_lof[\"anomaly\"] = anomaly_lof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7e94b8",
   "metadata": {},
   "source": [
    "## AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c0fd0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "logs_scaled = min_max_scaler.fit_transform(logs_as_vectors.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e93bb28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = AutoEncoder(output_units=logs_scaled.shape[1])\n",
    "ae.compile(loss='msle', metrics=['mse'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "16c46cc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20/20 [==============================] - 1s 4ms/step - loss: 0.0582 - mse: 0.1263\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0352 - mse: 0.0753\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0189 - mse: 0.0397\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0159 - mse: 0.0331\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0150 - mse: 0.0310\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0145 - mse: 0.0300\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0142 - mse: 0.0292\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0137 - mse: 0.0283\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0129 - mse: 0.0268\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0112 - mse: 0.0235\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0092 - mse: 0.0193\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0083 - mse: 0.0171\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0076 - mse: 0.0157\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0070 - mse: 0.0144\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0065 - mse: 0.0133\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0060 - mse: 0.0124\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0054 - mse: 0.0112\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0049 - mse: 0.0103\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0044 - mse: 0.0092\n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0040 - mse: 0.0084\n"
     ]
    }
   ],
   "source": [
    "ae_result = ae.fit(logs_scaled,\n",
    "                   logs_scaled,\n",
    "                   epochs=20,\n",
    "                   batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ab4be6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007617527150598186"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = find_threshold(ae, logs_scaled)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "fd54277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = get_predictions(ae, logs_scaled, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5cc00b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.11"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*len(predictions.loc[predictions == 1])/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "2bbbc7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ae = data.copy()\n",
    "data_ae[\"anomaly\"] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63aed0",
   "metadata": {},
   "source": [
    "## AE vs LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "1b8e94df",
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_anomaly_msgs = list(data_lof.loc[data_lof[\"anomaly\"] == 1].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9293a0f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ae_anomaly_msgs = list(data_ae.loc[data_ae[\"anomaly\"] == 1].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "50747391",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lof_diff_logs = []\n",
    "ae_diff_logs = []\n",
    "for x in list(set(ae_anomaly_msgs) - set(lof_anomaly_msgs)) + list(set(lof_anomaly_msgs) - set(ae_anomaly_msgs)) :\n",
    "    if (x in lof_anomaly_msgs) and (not x in ae_anomaly_msgs):\n",
    "        lof_diff_logs.append(x)\n",
    "        lof_diff += 1\n",
    "    if x in ae_anomaly_msgs and not x in lof_anomaly_msgs:\n",
    "        ae_diff_logs.append(x)\n",
    "        ae_diff +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "80ff95a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies, detected by LOF: 1228\n",
      "Anomaly percentage, detected by LOF: 12.28 %\n",
      "Anomalies, detected by AE: 1311\n",
      "Anomaly percentage, detected by AE: 13.11 %\n",
      "Number of messages, that LOF detected, but AE did not: 876\n",
      "Number of messages, that AE detected, but LOF did not: 539\n"
     ]
    }
   ],
   "source": [
    "print(\"Anomalies, detected by LOF:\", len(lof_anomaly_msgs))\n",
    "print(\"Anomaly percentage, detected by LOF:\", len(lof_anomaly_msgs)*100/len(data), \"%\")\n",
    "\n",
    "print(\"Anomalies, detected by AE:\", len(ae_anomaly_msgs))\n",
    "print(\"Anomaly percentage, detected by AE:\", len(ae_anomaly_msgs)*100/len(data), \"%\")\n",
    "\n",
    "print(\"Number of messages, that LOF detected, but AE did not:\", len(lof_diff_logs))\n",
    "print(\"Number of messages, that AE detected, but LOF did not:\", len(ae_diff_logs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14abf67f",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bb26c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
